1. Source files 
There are three programs used. 
Two for measuring performance, they are respectively kept under sourcefiles/noCombiner/ and sourcefiles/reducerUsedAsCombiner/.
one for analysing data to get interesting results, this is kept in a whole java project under sourcefiles/, the project name is BigDataAssignment.
The three java programs have a same name MeanTempJobConf.java

2.How to compile and generate JAR
a).First import the project BigDataAssignment into eclipse.
b)Set the compliance level to be 1.6 and choose default compliance setting.  Properties —> Java Compiler —> Compiler compliance level 
c).Press the run button, ignore the exceptions.
d).Right click the project name, choose Export —> Runnable Jar file —> Next —> Extract required libraries into generated JAR —>Finish

3.How to execute
Connect to ec2 and hadoop systems first —> upload the JAR file on Hue —>  copy the JAR file into hadoop system use hadoop fs -copyToLocal command —> execute the JAR file use hadoop jar command

4.Input
s3://aws-gsod/1944
s3://aws-gsod/1950
s3://aws-gsod/1961
s3://aws-gsod/2000
s3://aws-gsod/2011
s3://aws-gsod/2012
s3://aws-gsod/2013
s3://aws-gsod/2014
s3://aws-gsod/2015

5.output
All outputs are kept under outputs/
**For year 1950, there are two versions of outputs, one is generated by no combiner program, the other is generated by the program with a combiner also used as a reducer(not accuracy,only for performance measurement). For year 2011, there are also two versions of outputs, one is not accuracy and is generated by the program with a combiner also used as a reducer, the other is accurate and is generated by the program for analysing data.

6.log
All logs are kept under logfiles/
**For file name log_yyyy_xxnodes, yyyy represents which year’s data of the AWS GSOD, xx represents the number of nodes are used to run the job.
